# Paper Summaries

A personal collection of summaries from research papers I'm reading, especially in NLP and Deep Learning.

---

## ðŸ“„ Latest Summary

- [Attention Is All You Need](Attention-Is-All-You-Need.md)  
  Summary of the original Transformer paper by Vaswani et al., 2017 (NeurIPS). Introduces the attention-based architecture that powers modern language models like BERT and GPT.

-  (WIP) [BERT](BERT.md)
 
  Summary of the original BERT paper by Devlin et al., 2018 (NAACL). Introduces a deep bidirectional Transformer model pre-trained on large corpora for language understanding. Forms the foundation for many state-of-the-art NLP models and tasks.


---

More coming soon...
